---
title: "DATA 605 Final"
author: "Jeff Shamp"
date: "`r Sys.Date()`"
output:
  rmdformats::material:
    highlight: kate
---

```{r setup, echo=FALSE, cache=FALSE, message=FALSE}
library(knitr)
library(rmdformats)
library(tidyverse)
library(RCurl)
## Global options
options(max.print="100")
opts_chunk$set(echo=TRUE,
               tidy=TRUE,
               comment=NA,
               message=FALSE,
               warning=FALSE)
opts_knit$set(width=100)
```



# Problem 1

Using R, generate a random variable X that has 10,000 random uniform numbers from 1 to N, where N can be any number of your choosing greater than or equal to 6.  Then generate a random variable Y that has 10,000 random normal numbers with a mean of μ=σ=(N+1)/2.  


**We will set up some parameters, check that it works, and then calculate the probabilities.**

```{r}
set.seed(945945)
N<- 100
samp<-10000
mu<- (N+1)/2
sd<-mu
X<- runif(n = samp, min = 1, max = N)
Y<- rnorm(n = N, mean = mu, sd = sd)
df_1<- data.frame(X=X, Y=Y)
df_1 %>%
  ggplot() +
  geom_histogram(aes(X))
df_1 %>%
  ggplot() +
  geom_histogram(aes(Y))
```

## Probability{.tabset}

Calculate as a minimum the below probabilities a through c.  Assume the small letter "x" is estimated as the median of the X variable, and the small letter "y" is estimated as the 1st quartile of the Y variable.  Interpret the meaning of all probabilities. 

### Part A

P(X>x | X>y)

**We will define x and y first.**

```{r}
x<-quantile(df_1$X, 0.5)
y<-quantile(df_1$Y, 0.25)
paste("Median of X: ",x,"First quantile of Y: ",y)
```

**This is a conditional probability that X>x given that X>y. We want to know the probabiity that given X>y ~16, that is the chance that X>~50.**

```{r}
P_b_a<- df_1 %>% # prob of both conditions
  filter(X > x, X > y) %>%
  nrow()/samp
P_a<- df_1 %>% # prob that X is already > y
  filter(X > y) %>%
  nrow()/samp
P_a_b<- P_b_a / P_a
paste("Probability of X>x given that X>y = ",P_a_b)
```

**Here we see that X > y and X > x exactly half the time and X > y ~.85, so it makes sense that there is a slight increase in the probability that X > than the median given that X > the first quantile of y.**

### Part B

P(X>x, Y>y)

**This one is two separate varibales to it does not need a conditional term.**

```{r}
df_1 %>%
  filter(X > x, Y > y) %>%
  nrow()/samp
```

**So X is greater than the median of X AND Y is greater than it's first quantile, 37.68% of the time.**

### Part C

P(X<x | X>y)

**This is will be a similar set up to the part A. When is X less than it's median given that it is greater than the first quantile of Y.**

```{r}
P_b_a<- df_1 %>%
  filter(X < x, X > y) %>%
  nrow()/samp

P_a<- df_1 %>%
  filter(X > y) %>%
  nrow()/samp

P_a_b<- P_b_a/P_a
paste("Probability that X < x given that X > y = ",P_a_b)
```

## Marginal and Joint Probabilities

Investigate whether P(X>x and Y>y)=P(X>x)P(Y>y) by building a table and evaluating the marginal and joint probabilities.


**I'm going to do this brute force style, calculate each probability and then corral them into a dataframe to show.**

```{r}
# filter for conditions
XY_ls_xy<- df_1 %>% filter(X < x, Y < y) %>% nrow()/samp
X_gt_x_Y_ls_y <- df_1 %>% filter(X > x, Y < y) %>% nrow()/samp
X_ls_x_Y_gt_y <- df_1 %>% filter(X < x, Y > y) %>% nrow()/samp
XY_gt_xy<- df_1 %>% filter(X > x, Y > y) %>% nrow()/samp

prob_table<- as.data.frame(matrix(c(XY_ls_xy, X_gt_x_Y_ls_y,
                                    X_ls_x_Y_gt_y, XY_gt_xy),
                                  byrow = TRUE, 
                                  nrow = 2))
prob_table$Total<- apply(prob_table,1,sum) # row wise sums
prob_table[3,"V1"]<-sum(prob_table$V1, na.rm = TRUE) # get column sums
prob_table[3,"V2"]<-sum(prob_table$V2, na.rm= TRUE)
prob_table[3,"Total"]<-sum(prob_table$Total, na.rm = TRUE)
rownames(prob_table)<- c("Y < y", "Y > y", "Total")
DT::datatable(prob_table %>% rename("X < x" = "V1", "X > x" = "V2"),
              extensions = c('FixedColumns',"FixedHeader"),
              options = list(scrollX = FALSE, 
                         paging=FALSE,
                         fixedHeader=FALSE,
                         dom  = 't'))
```

**There is a small difference between the P(X>x and Y>y) and P(X>x)\*P(Y>y), but the difference is 0.0018**


## Independence Test

Check to see if independence holds by using Fisher’s Exact Test and the Chi Square Test.  What is the difference between the two? Which is most appropriate?

**We are now to perform two tests on the contingency table for the above data to determine if X and Y are independent. First, let's get the contingency table, then we can easily perform the tests. Fisher's test works for all samples so it can be a fine tool for this job. Chi-Square can also be used for independence and is works best with a large sample, which we have. A key difference between these tests is that Fisher's is an exact calculation of p-value and Chi squared is an approximation from the sampling distribution.**

**In both of these tests we see that with a very large p-value we fail to reject the null hypothesis. These variables are independent, which I would expect given that they come from two different, random, independent variables.**


```{r}
count_table<- prob_table[1:2,1:2]*samp # use prob_table to get counts
DT::datatable(count_table %>% rename("X < x" = "V1", "X > x" = "V2"),
              extensions = c('FixedColumns',"FixedHeader"),
              options = list(scrollX = FALSE, 
                         paging=FALSE,
                         fixedHeader=FALSE,
                         dom  = 't'))
```


```{r}
count_table<- count_table %>% rename("X < x" = "V1", "X > x" = "V2")
fisher.test(count_table)
```


```{r}
chisq.test(count_table)
```


# Problem 2

Kaggle username : jeffshamp 

Kaggle Competition Score : 0.17735



```{r}
# Just showing the first six columns
d<- getURL("https://raw.githubusercontent.com/Shampjeff/cuny_msds/master/DATA_605/final_project/train.csv")
train_df<- read.csv(text=d, stringsAsFactors = FALSE)
DT::datatable(train_df[,1:6], 
         extensions = c('FixedColumns',"FixedHeader"),
          options = list(scrollX = TRUE, 
                         paging=TRUE,
                         fixedHeader=TRUE,
                         pageLengh=7))
```


## Descriptive and Inferential Statistics{.tabset}

### Descriptive Statistics

Provide univariate descriptive statistics and appropriate plots for the training data set.

```{r}
print("GrLivArea: Above grade (ground) living area square feet")
summary(train_df$GrLivArea)
print("Sale Price - target variable")
summary(train_df$SalePrice)
print("TotalBsmtSF: Total square feet of basement area")
summary(train_df$TotalBsmtSF)
print("LotArea: Lot size in square feet")
summary(train_df$LotArea)
```

```{r}
train_df %>%
  ggplot(aes(x=SalePrice)) + 
  geom_histogram(aes(y=..density..),
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(title="Sale Price Histrogram with Density")

train_df %>%
  ggplot(aes(x=GrLivArea)) + 
  geom_histogram(aes(y=..density..), 
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(title="Above Ground Living Area Histrogram with Density")

train_df %>%
  ggplot(aes(x=TotalBsmtSF)) + 
  geom_histogram(aes(y=..density..), 
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(title="Total Basement Area Histrogram with Density")

train_df %>%
  ggplot(aes(x=LotArea)) + 
  geom_histogram(aes(y=..density..),
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(title="Total Lot Area Histrogram with Density")
```


### Scatterplot Matrix

Provide a scatterplot matrix for at least two of the independent variables and the dependent variable. 

**A plot of the Overall Quality variable is included below. Looks to be quadratic.**

```{r}
pairs(train_df %>% dplyr::select(SalePrice, GrLivArea,
                                 TotalBsmtSF, LotArea, OverallQual))
```

```{r}
train_df %>%
  ggplot(aes(x=factor(OverallQual), y=SalePrice, fill=factor(OverallQual))) +
  geom_boxplot() +
  labs(x= "Overall Quality", title="Sale Price by Overall Quality") +
  theme(legend.position = "top") +
  guides(fill=guide_legend(title="Quality Scale"))
```



### Correlation Matrix

Derive a correlation matrix for any three quantitative variables in the dataset. 

```{r}
cor_matrix<- train_df %>%
  dplyr::select(SalePrice, GrLivArea, TotalBsmtSF) %>%
  cor() %>%
  as.matrix()
cor_matrix
```

**Lot Area appears to be the low performer, probably because of expensive apartments in an urban area like Boston.**

### Hypothesis Testing

Test the hypotheses that the correlations between each pairwise set of variables is 0 and provide an 80% confidence interval. Discuss the meaning of your analysis. 

Correlation Test with 80% confidence interval for GrLivArea
```{r}
cor.test(train_df$GrLivArea, train_df$SalePrice, conf.level = 0.8)
```

Correlation Test with 80% confidence interval for Total Basemnet SqFt
```{r}
cor.test(train_df$TotalBsmtSF, train_df$SalePrice, conf.level = 0.8)
```

Correlation Test with 80% confidence interval for Lot Area
```{r}
cor.test(train_df$GrLivArea, train_df$TotalBsmtSF, conf.level = 0.8)
```


**We see that for each pairwise comparison the p-value is very small, so can safely reject the null hypothesis that each correaltion is actually zero.** 

Would you be worried about familywise error? Why or why not?

**Regarding, familywise error it seems likely that some of the variables will fail to reject the null. Can calculate an adjusted p-value using the following $1-(1-\alpha)^{m}$ where is the number of test we perform.**

```{r}
1-(1-0.05)^3
```

**So it is indeed vary likely to wrongly find a "significant".**

## Linear Algebra and Correlation{.tabset}

### Invert Correlation Matrix

Invert your correlation matrix from above. (This is known as the precision matrix and contains variance inflation factors on the diagonal).

```{r}
precision_matrix<- solve(cor_matrix)
precision_matrix
```


### Precision and Correlation Matricies 

Multiply the correlation matrix by the precision matrix, and then multiply the precision matrix by the correlation matrix. 

**Any matrix multiplied by it's inverse should be the indentity matrix.**

```{r}
round(cor_matrix %*% precision_matrix)
```

**The same must be true for the precision times correlation.**

```{r}
round(precision_matrix %*% cor_matrix)
```


### LU Decomposition

Conduct LU decomposition on the matrix.

**Homegrown LU Decomposition function.**

```{r}
matrix.factorize<- function(input_mat){
  mat_L<-diag(dim(input_mat)[1])
  row_idx<-1
  for(j in 1:(dim(input_mat)[2]-1)){
    for(i in 1:(dim(input_mat)[1]-row_idx)){
      mat_L[i+row_idx,j]<-(input_mat[i+row_idx,j] /
                             input_mat[j,j])
      input_mat[i+row_idx,]<-((-1*mat_L[i+row_idx,j]) *
                                input_mat[row_idx,]) +
                                input_mat[i+row_idx,]
    }
  row_idx<-row_idx+1
  }
  return(list(input_mat, mat_L))
}
decomp<-matrix.factorize(cor_matrix)
```

```{r}
matrix(decomp[[1]],nrow=3)
decomp[[2]]
decomp[[2]]%*%decomp[[1]]
```

**We see that this is the original correlation matrix when L and U multiplied.**

## Calculus-Based Probability & Statistics{.tabset}

Many times, it makes sense to fit a closed form distribution to data.  Select a variable in the Kaggle.com training dataset that is skewed to the right, shift it so that the minimum value is absolutely above zero if necessary.

**Recall `GrLivArea` as right skewed. We will apply a log transform this data**

```{r}
train_df %>%
  ggplot(aes(x=GrLivArea)) + 
  geom_histogram(aes(y=..density..),
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(title="Above Ground Living Area Histrogram with Density")

train_df %>%
  ggplot(aes(x=log(GrLivArea))) + 
  geom_histogram(aes(y=..density..),
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(title="Above Ground Living Area Histrogram with Density")

train_df %>%
  ggplot(aes(sample=GrLivArea)) +
  stat_qq() +
  stat_qq_line() + 
  labs(title="QQ Plot - GrLivArea")

train_df %>%
  ggplot(aes(sample=log(GrLivArea))) +
  stat_qq() +
  stat_qq_line() + 
  labs(title="QQ Plot - Log Transform of GrLivArea")
```


### Optimum Lambda

Load the MASS package and run fitdistr to fit an exponential probability density function. Find the optimal value of λ for this distribution

```{r}
library(MASS)
exp_liv_area<- fitdistr(train_df$GrLivArea, "exponential")
exp_liv_area
```


### Sampling with Lambda

Take 1000 samples from this exponential distribution using this value (e.g., rexp(1000, λ)).  Plot a histogram and compare it with a histogram of your original variable.   

```{r}
exp_sample<-rexp(1000, exp_liv_area$estimate)

ggplot(as.data.frame(exp_sample), aes(x=exp_sample)) +
  geom_histogram(aes(y=..density..), 
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(title="Exponential Distribution for GrLivArea with Density")

train_df %>%
  ggplot(aes(x=GrLivArea)) + 
  geom_histogram(aes(y=..density..),
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(title="Above Ground Living Area Histrogram with Density")

```


### PDF, CI, and Percentiles

Using the exponential pdf, find the 5th and 95th percentiles using the cumulative distribution function (CDF).   Also generate a 95% confidence interval from the empirical data, assuming normality.  Finally, provide the empirical 5th percentile and 95th percentile of the data.  Discuss.

**5th and 95th percentiles from the exponential distribution.**

```{r}
qexp(0.05, rate=exp_liv_area$estimate)
```

```{r}
qexp(0.95, rate=exp_liv_area$estimate)
```

**CI with assumed normality from the actual dataset.**

```{r}
SE<- sd(train_df$GrLivArea)/ (sqrt(length(train_df$GrLivArea)))
lower<- mean(train_df$GrLivArea) - 1.960*SE
upper<- mean(train_df$GrLivArea)+ 1.960*SE
paste("95% CI for above ground living area (GrLivArea): ",
      round(lower,3), " - ",round(upper,3) )
```

**Empircial 5th and 95th percentiles**

```{r}
quantile(train_df$GrLivArea, 0.05)
quantile(train_df$GrLivArea, 0.95)
```

**We see that this data is, in fact, skewed to the right considering the 5th/95th percentiles and the true mean and it's 95% CI. The exponential distribution seems like an un-needed example in this context. Unless the was some kind of evidence that the data "shuold" be this way.**



## Modeling

Build some type of multiple regression  model and submit your model to the competition board.  Provide your complete model summary and results with analysis. 


### General Approach

**I'll keep this relatively simple. Let's look at above ground area, below ground area, garage size and number of cars, and two measures of quality (house and garage). I think that several of these predictors could use a log transform to make them more normalized. Also it might be helpful to have at least one or two categorical variables; Overall Quality, and GarageQual seem like a nice candidates. Garage quality will take work to get the dummy variables. Additionally, the overall quality variable appears to quadratic so we will inlude that in the model.**

### Missing Values

**Let's first check for missing values with our set of predictors.**

```{r}
train_df %>%
  dplyr::select(GrLivArea, SalePrice, TotalBsmtSF, 
                OverallQual, GarageQual, GarageCars) %>%
  summarise_all(funs(sum(is.na(.))))

train_df<-train_df %>%
  dplyr::select(GrLivArea, SalePrice, TotalBsmtSF,
                OverallQual, GarageQual, GarageCars)
```


**We need to fill some values for Garage Quality. Let's look at the most frequent rating.**

```{r}
train_df %>%
  ggplot(aes(x=GarageQual)) +
  geom_bar()

```

**We will fill NA's with `TA` rating in garage quality.**

```{r}
train_df<- train_df %>%
  mutate(GarageQual = replace_na(GarageQual, "TA"))
```


### Check for Outliers

**I believe we have some outliers in GrLiveArea.**

```{r}
train_df %>%
  dplyr::select(SalePrice, GrLivArea) %>%
  ggplot(aes(x=GrLivArea, y=SalePrice)) +
  geom_point() + 
  labs(x="Above Ground Living Area",
       y="Sale Price",
       title="Sale Price v. Above Ground Living Area")
```


```{r}
z_score<- scale(train_df$GrLivArea)
train_df$GrLivArea_Z<- z_score
max(z_score)
```

```{r}
train_df<- train_df %>%
  filter(GrLivArea_Z < 5)
```

### Categorical Variables

**We need to convert the garage quality variable from a categorical to dummy numerical values.**

```{r}
library(caret)
dmy <- dummyVars(" ~ .", data = train_df, fullRank = T)
train_df <- data.frame(predict(dmy, newdata = train_df))

glimpse(train_df)
```

### Multiple Regression

**Let's try out regression and analyze the results. We tried garage area as well but found that it is not a better predictor than garage cars. Additionally, the categories for `GarageQual` have been deleted as the result of backward selection for predictors whose p-values were above 0.05 critical values (fails to reject null hypothesis for non-zero slope). In the end we have a relatively simple model, this produces the best results on the competition.**

```{r}
lin_reg<- lm(data=train_df, log(SalePrice) ~ log(GrLivArea)+ TotalBsmtSF + 
               I(OverallQual^2) + GarageCars)  

summary(lin_reg)
```

```{r}
lin_reg %>%
  ggplot(aes(x=resid(lin_reg))) + 
  geom_histogram(aes(y=..density..), 
                 fill="lightblue",color="black", alpha=0.7) +
  geom_density(color="red") +
  labs(x="Residuals", y="Count or Density",
       title="Histogram of Residuals with Density")
```


### Test Data

**Bring in the test data and predict new values.**

```{r}
d<- getURL(
"https://raw.githubusercontent.com/Shampjeff/cuny_msds/master/DATA_605/final_project/test.csv")
test_df<- read.csv(text=d, header = TRUE, stringsAsFactors = F)
test_df<-test_df %>%
  dplyr::select(Id, GrLivArea, TotalBsmtSF, 
                OverallQual, GarageCars)
dmy <- dummyVars(" ~ .", data = test_df, fullRank = T)
test_df <- data.frame(predict(dmy, newdata = test_df))
glimpse(test_df)
```

**Predict the Sale Price values and clean up the formatting.**

```{r}
preds<-predict(lin_reg, test_df)
preds<-as.data.frame(preds)
rownames(preds)<-NULL
```

**Prepare the Kaggle Data Submission.**

```{r}
submission_data<- as.data.frame(cbind(test_df$Id, exp(preds)))
submission_data<- submission_data %>%
  rename(Id = "test_df$Id", SalePrice = "preds") %>%
  mutate(SalePrice = replace_na(SalePrice, median(SalePrice, na.rm =T)))

DT::datatable(head(submission_data),
         extensions = c('FixedColumns',"FixedHeader"),
          options = list(scrollX = TRUE,
                         paging=TRUE,
                         fixedHeader=TRUE,
                         dom="t"))
```


```{r}
write.csv(submission_data, file = "Kaggle_Submit_Shamp.csv", 
          quote = FALSE, row.names = FALSE)
```

**Not a great score, but we only considered a few variable of the many options. We were also constrained to multiple regression.**

**Kaggle username : jeffshamp**

**Kaggle Competition Score : 0.17735**




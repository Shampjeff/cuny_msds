{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lyric-falls",
   "metadata": {},
   "source": [
    "# Shamp Week 2 HW Data 620"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-twenty",
   "metadata": {},
   "source": [
    "### Directions\n",
    "\n",
    "1) Load a graph database of your choosing from a text file or other source.  If you take a large network dataset from the web (such as from Stanford Large Network Dataset Collection), please feel free at this point to load just a small subset of the nodes and edges.\n",
    "\n",
    "2) Create basic analysis on the graph, including the graphâ€™s diameter, and at least one other metric of your choosing.  You may either code the functions by hand (to build your intuition and insight), or use functions in an existing package. \n",
    "\n",
    "3) Use a visualization tool of your choice (Neo4j, Gephi, etc.) to display information.\n",
    "\n",
    "4) Please record a short video (~ 5 minutes), and submit a link to the video in advance of our meet-up.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informative-transmission",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-consent",
   "metadata": {},
   "source": [
    "We will be using a data set from SNAP, listed above. It is the Reddit discussion, reply data set and we will subset it to a smaller size. The data is stored in a JSON file, so we will need to load that file and truncate it to the first 100 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "liked-encoding",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as net\n",
    "from networkx.readwrite import json_graph\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unauthorized-seven",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reddit_edges.json\", \"r\") as read_file:\n",
    "    data = json.load(read_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "defensive-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {f\"{k}\": data[f\"{k}\"] for k in range(100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "neutral-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "hw = net.Graph()\n",
    "hw.add_nodes_from(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "continued-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {}\n",
    "for k, v in data.items():\n",
    "    hw.add_edges_from([tuple(i) for i in v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-belfast",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-apparel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

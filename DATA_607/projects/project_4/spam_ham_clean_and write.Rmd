---
title: "Spam vs Ham - Project 4 - Data 607"
author: "Jeff Shamp"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: vignette
---

```{r setup, include=FALSE}
library(tidyverse)
library(e1071)
library(tm)
library(tidytext)
library(caret)
library(xgboost)
library(RMySQL)
```

# Cleaning and Data Preparation

First, we will import, clean, and process the data for classification.

## Datasets

I found a large library of datasets [here](https://www.cs.bgu.ac.il/~elhadad/nlp16/spam_classifier.html). Below is an excerpt from the site regarding a library of resources. 

The email spam messages are collected from:

The ENRON [email archive](http://www.aueb.gr/users/ion/data/enron-spam/)
The Apache [Spam Assassin dataset](https://spamassassin.apache.org/publiccorpus/)
To make the work simpler, the two datasets are put into a single zip file [here](http://www.cs.bgu.ac.il/~elhadad/nlp16/spam.zip (107MB, contains about 60K files).

The SMS dataset is from:

[SMS data](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)

For Reference on class labels

SOURCES = [
    ('data/spam',        SPAM),
    ('data/spam_2,       SPAM),
    ('data/easy_ham_2,   HAM),
    ('data/easy_ham',    HAM),
    ('data/hard_ham',    HAM),
    ('data/beck-s',      HAM),
    ('data/farmer-d',    HAM),
    ('data/kaminski-v',  HAM),
    ('data/kitchen-l',   HAM),
    ('data/lokay-m',     HAM),
    ('data/williams-w3', HAM),
    ('data/BG',          SPAM),
    ('data/GP',          SPAM),
    ('data/SH',          SPAM)
]


## Read Files

I divided the files into two sets for spam and ham. There are ~67K files spread out over many folders, I will load them locally and read in the files. The zip files containing the data is on [github](https://github.com/Shampjeff/cuny_msds/tree/master/DATA_607/data).

**Reproduction Note: Follow the link above to my github repo and download the zip file and load the files locally. Set the ham and spam paths to your local directory. These are large files with many subdirectories so it will be faster and easier to unzip locally and read the files.**

```{r}
spam_path = "/Users/jeffshamp/Downloads/data/spam"
ham_path = "/Users/jeffshamp/Downloads/data/ham"
```

This function crawls through the files and extracts the messages in there raw form. 

```{r read_file_func}
make.data.frame<- function(path, class){
  # Dig through the directories for messages
  files <- list.files(path=path, 
                      full.names=TRUE, 
                      recursive=TRUE)
  # Read a file once directories are gone
  message<-lapply(files, function(x) {
    text_body<-read_file(x)
    })
  # Add to dataframe and assign "id" column
  message<-unlist(message)
  data<-as.data.frame(message)
  data$class<-class
  return (data)
} 
```

Make SPAM and HAM dataframes and bind them.

```{r make_dataframes}
data<-make.data.frame(spam_path, class="SPAM")
data<-rbind(data, make.data.frame(ham_path, class="HAM"))
```

The SMS dataset can be bound as well. **Again, please see the zip files on [github](https://github.com/Shampjeff/cuny_msds/tree/master/DATA_607/data) to load the data to your machine.** 

```{r}
sms_data<- as.data.frame(
  read_lines(
  "/Users/jeffshamp/Downloads/smsspamcollection/SMSSpamCollection"
    ))
names(sms_data)<-"lines"
sms_data<-sms_data %>%
  separate(col = lines, into = c("class", "message"), sep = "\t") %>%
  mutate(class = str_to_upper(class)) %>%
  mutate(message = factor(message))

data<-rbind(data, sms_data)
```

### Best of Text Messages

Let's take a look at the messages and see what kind of divine prose our writers produce. 

> "Shall I compare thee to a summer’s day? <br />
> Thou art more lovely and more temperate: <br />
> Rough winds do shake the darling buds of May, <br />
> And summer’s lease hath all too short a date; <br />
> Sometime too hot the eye of heaven shines, <br />
> And often is his gold complexion dimm'd; <br />
> And every fair from fair sometime declines, <br />
> By chance or nature’s changing course untrimm'd; <br />
> But thy eternal summer shall not fade, <br />
> Nor lose possession of that fair thou ow’st; <br />
> Nor shall death brag thou wander’st in his shade, <br />
> When in eternal lines to time thou grow’st: <br />
>  So long as men can breathe or eyes can see, <br />
>  So long lives this, and this gives life to thee." <br />

The above was [not a text message](https://www.poetryfoundation.org/poems/45087/sonnet-18-shall-i-compare-thee-to-a-summers-day) from SMS dataset. Our most verbose actor had the following, similar take on love...

```{r}
best_of<-sms_data %>%
  mutate(length = str_length(message)) %>%
  arrange(desc(length))
str_split(best_of[1,'message'], "[.]")[[1]]
```

## Text Clean Up

Some classifiers (like XGBoost) need the target class to numerical, others do not (like Naive Bayes). I'll make both so that we can try out different modeling methods. 

```{r}
data_spam<-data %>%
  filter(class == "SPAM") %>%
  mutate(target = 1)
data_ham<- data %>%
  filter(class == "HAM") %>%
  mutate(target = 0)
data<-rbind(data_spam, data_ham)
data$id <- 1:nrow(data)

DT::datatable(data %>%
              count(class, target),
         extensions = c('FixedColumns',"FixedHeader"),
          options = list(scrollX = TRUE,
                         paging=TRUE,
                         fixedHeader=TRUE))
```

Now we need to clean the text files. Below we remove the html formatting, all punctuation, new lines, and digits from the text. We will also remove all stop words in the tidy lexicon. Maybe we can remove more later is the model isn't up to par. 

```{r clean_text}
data<-data %>%
  mutate(message= str_remove_all(message, pattern = "<.*?>")) %>%
  mutate(message= str_remove_all(message, pattern = "[:digit:]")) %>%
  mutate(message= str_remove_all(message, pattern = "[:punct:]")) %>%
  mutate(message= str_remove_all(message, pattern = "[\n]")) %>%
  mutate(message= str_to_lower(message)) %>%
  unnest_tokens(output=text,input=message,
                token="paragraphs",
                format="text") %>%
  anti_join(stop_words, by=c("text"="word"))
```

These are currently ordered, as in all the Ham is first followed by all the Spam. We need to shuffle this order for later use in the train, test splits. 

```{r, warning=FALSE}
set.seed(9450)
# randomize index
row_shuffle <- sample(nrow(data))
# reorder index
data<-data[row_shuffle,]

DT::datatable(data[1:5,c("class","id")],
         extensions = c('FixedColumns',"FixedHeader"),
          options = list(scrollX = TRUE,
                         paging=TRUE,
                         fixedHeader=TRUE,
                         pageLength = 5))
```

## Document Term Matrix

We need to vectorize the words in the corpus of messages and `tm` package seems to be a good job of handling that. We are also stemming the words while making the matrix since it seemed easier to do here than using tidyverse. The `tm` package offer a lot of really good functionality for this kind of work. 

```{r}
text_corpus <- VCorpus(VectorSource(data$text))
```

Now that this is a corpus we can produce a Document Term Matrix, which is a bag-of-words vectorizer for each message in the dataset. Without removing any sparse terms (below) this will produce a huge matrix that that tens of thousands of columns. Each column represents the count frequency of each word in the corpus. 

```{r}
# For tokens by message
text_dtm <- DocumentTermMatrix(text_corpus, control =
                                 list(stemming = TRUE))
dim(text_dtm)
```

We will reduce the number of columns to only the words that show up frequently (and are not stopwords). Reducing the dataset is very helpful in improving the predictive power of the model. Too many words "confuses" the algorithm and generally returns an overly simple model. For example, when I ran this without any term frequency filtering, every model just labeled all messages as SPAM. 

We will also add a TFIDF score to each word to weight it's relative frequency within each message. This is a great way to feature engineer without adding columns. Now the matrix shows word frequency by corpus and message. 

```{r, warning=FALSE}
# Filter out sparse terms
text_dtm <- removeSparseTerms(text_dtm,sparse = 0.92)

# Create TFIDF score
text_dtm <- as.data.frame(as.matrix(weightTfIdf(text_dtm)))
                             
dim(text_dtm)
```

Big reduction in columns, that's good as it filters down to only the (most likely) important words.


## Train, Test, Split

Normally, I would do 5-fold cross validation for evaluate an ML model, but the spam/ham problem is well solved and understood. My guess is that Naive Bayes Classifier will probably do a sufficient job classifying these with a single split. This is a large dataset and will take some time to process, CV will only add to that computational load. We will do a 75/25 train/test split from the shuffled classes. 


```{r}
test_split<-round(.25*dim(text_dtm)[1])

test_text<-text_dtm[1:test_split,]
train_text<-text_dtm[(test_split+1):dim(text_dtm)[1],]

test_target<-data$class[1:test_split]
train_target<-data$class[(test_split+1):dim(data)[1]]
```


# Modeling

A few points on the modeling process.

1. I tried several models and Naive Bayes and XGB worked the best. Normally, when modeling is it a good idea to try several options to determine best performance and speed. I have standard practice that I like to use but it is in Python. [See this repo for a Python Package](https://github.com/Shampjeff/Personal_Projects/blob/master/ml_test_tools_dev.py) I built for model testing and evaluation. It comes with a [vignette](https://github.com/Shampjeff/Personal_Projects/blob/master/ml_test_tool_example.ipynb) on how to use. Having something to evaluate and store model results is very helpful to show the progress of the model development. 
2. The Naive Bayes Classifier worked well, but still needed several testing iterations to get to 91% accuracy. Filtering the sparse terms was essential and brought the classifier from 52% accuracy to 78%. Second, a little more sparse filtering and TFIDF scoring pulled the classifier up to 91% accuarcy. 
3. XGBoost is so good and very very quick to run on this dataset. Much faster than Naive Bayes with better results. For direct comparison, I used the same train/test sets as the Naive Bayes so the feature engineering needed for the NB also likely helped XGB. 

## Naive Bayes Classifier

The `e1071` package has a Naive Bayes classifier. It needs the input training/test set to be a matrix and the target to be a vector. We can produce a confusion matrix to show the models performance. That confusion matrix comes the `caret` package. 

```{r}
# Initialize Naive Bayes
text_clf <- naiveBayes(as.matrix(train_text), as.factor(train_target))
```


```{r}
# Make predictions on test set
preds <- predict(text_clf, as.matrix(test_text))
```


```{r}
# Create confusion matrix
confusionMatrix(data = preds, reference = as.factor(test_target),
                positive = "SPAM", dnn = c("Prediction", "Actual"))
```

Awesome. There is some tricky spam in the sms dataset. Without the SMS dataset, this classifier had 97% accuaracy. Now let's look at XGB. 

## XGBoost

XGB is a powerful tree boosted classifier that really sorted the last few, tricky SPAM/HAM messages. 

```{r}
test_split<-round(.25*dim(text_dtm)[1])

test_text<-text_dtm[1:test_split,]
train_text<-text_dtm[(test_split+1):dim(text_dtm)[1],]

# XGB needs numerical labels so we use the target column
test_target<-data$target[1:test_split]
train_target<-data$target[(test_split+1):dim(data)[1]]

# Vanilla parameters work pretty well in most cases
xgb <- xgboost(data = as.matrix(train_text), 
               label = as.vector(train_target),
               max.depth = 7, eta = 1, 
               nthread = 2, nrounds = 2,
               objective = "binary:logistic")

# Predict
xg_pred <- predict(xgb, as.matrix(test_text))

# Convert probabilities to binary
xg_pred<- ifelse(xg_pred >0.5, 1,0)

# Evaluate
confusionMatrix(data = factor(xg_pred, levels=c(1,0)),
                reference = factor(test_target, levels=c(1,0)),
                positive = "1", dnn = c("Prediction", "Actual"))
```

Wow. So much better - down to ~300 wrongly classified messages out of ~16K in the test set. In addition to being a scary good classifier the `xgboost` package has some nice features for interpreting how the model works. Below the feature importance plot showing the highest ranking model features for SPAM detection. 

```{r fig.height=8, fig.width=9}
importance_matrix <- xgb.importance(model = xgb)
xgb.plot.importance(importance_matrix = importance_matrix)
```

Here is the matrix of importance factors for the XGBoost model. 

```{r}
DT::datatable(importance_matrix,
         extensions = c('FixedColumns',"FixedHeader"),
          options = list(scrollX = TRUE,
                         paging=TRUE,
                         fixedHeader=TRUE,
                         pageLength = 10))
```



We see that common meta data tags have the largest importance in this model with normal word stems rounding out the top 15 predictors. Even a relly good spam message usually comes from a strange looking address or has some irregular formatting. 







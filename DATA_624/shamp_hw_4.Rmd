---
title: "624 HW4"
author: "Jeff Shamp"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: architect
    highlight: github
---

# Question KJ 3.1
## Question

This data set is related to glass identification. The data consist of 214 glass samples labeled as one
of seven class categories. There are nine predictors, including the refractive
index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
library(mlbench)
data(Glass)
str(Glass)
```

 (a) Using visualizations, explore the predictor variables to understand their
 distributions as well as the relationships between predictors.
 (b) Do there appear to be any outliers in the data? Are any predictors skewed?
 (c) Are there any relevant transformations of one or more predictors that might
 improve the classification model?
 
## Answer

The answer will combine parts (a), (b), and (c) into one narrative for a more wholistic
approach to what is a sprawling process. 

### Variable Distributions

The first step is to understand the data. The call to `str` in the question
is very valuable. It tells us that we can naÃ¯vely expect that each of the values
comes from a continuous distribution, since they are coded as floats/doubles.
The exception is the `Type` variable which is a factor which takes 6 discrete
values. If we wanted to use this as a predictor in a regression we would have to
consider using dummy variables.

The first visualization step is usually to plot the distribution of
each of the variables. Below are histograms for each of
the 10 variables. To make life a bit simpler, the factor `Type` will be
converted to an integer. 

```{r warning=FALSE, message=FALSE}
Glass %>%
  pivot_longer(!Type,
               names_to = "key", 
               values_to = "value") %>%
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~key, scales = 'free')
```

The plots show that some of the variable such as Potassium (**K**), Barium
(**Ba**), and Iron (**Fe**) have classic very right-tailed skew distributions.
Others, such as Magnesium (**Mg**), Calcium (**Ca**), and the refractive index
(**RI**) have slightly right-tailed distributions with noticeable observations
below the mode. Still others, such as Silicon (**Si**), Sodium (**Na**), and
Aluminum (**Al**) are borderline symmetrical. Lastly, Magnesium (**Mg**) doesn't
fit into any category. Rather it has peaks at the ends and dips in the
middle. Maybe this is a beta distribution, but it isn't clear at this point.
**Type** doesn't have an immediately recognizable shape either, but as it is a
categorical variable, so it's of little concern. We can check for skewness using
the sample skew parameter.

    
```{r, message=FALSE}
library(moments)
Glass %>%
  pivot_longer(!Type,
               names_to = "key", 
               values_to = "value") %>%
  group_by(key) %>%
  summarise(skewness = skewness(value)) %>%
  arrange(desc(skewness))
```

As seen in the histograms, `Ca`, `K`, and `Ba` show the heaviest right skew.
We will likely need transformations to handle this variables. 

### Transforms
#### Scaling and Centering

The next step, with continuous data at least, is to look at the data after
transformations. The simplest is the Z-scaling of the data, which subtracts the
mean and divides by the standard deviation of each variable. This sets all means
to 0 and puts the variability on the same scale. This which will obviously not
be applied to the `Type` variable.

```{r fig.width = 9L, fig.height = 8L, message=FALSE}
scaled<- 
  Glass %>%
  select(!Type) %>%
  scale() %>%
  as.data.frame()

glass_scale<- cbind(Glass$Type, scaled)

glass_scale %>%
  rename(Type = `Glass$Type`)%>%
  pivot_longer(!Type,
               names_to = "key",
               values_to = "value") %>%
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~key, scales = 'free')
```

Unfortuately, this did not help matters much and the skew not at all, which
stands to reason, as centering and scaling do not materially address skew.
I find that scaling and centering are, in the real world, fairly useless. 
They add another layer of complexity for non-technical people, and often do
not address things like skew of distribution irregularities. 

#### Logarithmic Transformation

A transformation often used to address skew in particular is to log the data.
Again, this is inappropriate for the `Type` variable.

```{r fig.width = 9L, fig.height = 8L, message=FALSE, warning=FALSE}
logged<- 
  Glass %>%
  select(!Type) %>%
  log() 

glass_log<- cbind(Glass$Type, logged)

glass_log %>%
  rename(Type = `Glass$Type`) %>%
  pivot_longer(!Type,
               names_to = "key", 
               values_to = "value") %>%
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~key, scales = 'free')

glass_log %>%
  rename(Type = `Glass$Type`) %>%
  pivot_longer(!Type,
               names_to = "key", 
               values_to = "value") %>%
  group_by(key) %>%
  summarise(skewness = skewness(value)) %>%
  arrange(desc(skewness))
```

Now, some of the distributions begin to look more symmetrical. Maybe we could
try a lognormal distribution. Magnesium, however, becomes
heavily left-tailed. I believe this is seen in gamma distributions
and in beta distributions where both shape parameters are nearly equal.
This confirms to some extent our observation above.

Note that many of the variables had observations of 0, which returns `-Inf` when
logged. This is the reasons for the `Nan`s in the skew calculations. This also
indicates that a log-transform may not be the best for those variables.

#### Other Transformations

Other transformations include power transformations and the Box-Cox family of
transforms with parameter \(\lambda\):
\[
x^* =
\begin{cases}
\frac{x^\lambda - 1}{\lambda}\qquad &\textrm{if }\lambda \neq 0\\
\log(x) &\textrm{if }\lambda = 0
\end{cases}
\]

We have used this extensively already in class, so let's see if BoxCox can
be of use here. My guess is that it will only work for handful of elements.
We will look at the lambda values first and determine where to go from there.


```{r, message=FALSE, warning=FALSE}
 Glass %>%
  pivot_longer(!Type,
               names_to = "key", 
               values_to = "value") %>%
  group_by(key) %>%
   summarise(lambda_key = forecast::BoxCox.lambda(value))
```


This is likely to do little for our data. We have inverses for `RI` and `Si`,
a power of one for `Mg`, so that doesn't help. The cubic root for `Al`, and 
possibly it's inverse for `Ca` will likely help, but that's about it. 
As such, we will pass on Box Cox. 

### Variable Correlations

To visually inspect correlation, the simplest approach is to create scatterplots
of each variable against each other.

```{r, fig.width = 9L, fig.height = 8L, warning=FALSE, message=FALSE}
GGally::ggpairs(Glass)
```

Not much in the way of overwhelming correlations. However, there are seem to be
some relationships. For example,  `Refractive Index` seems to be highly positively
correlated with `Ca` and somewhat negatively correlated with `Si`.

### Outliers

The scatterplot doesn't show strong evidence of outliers. However, boxplots or
violin plots are often useful in identifying outliers.

```{r, fig.width = 9L, fig.height = 8L}
Glass %>%
  pivot_longer(!Type,
               names_to = "key", 
               values_to = "value") %>%
  ggplot(aes(key, value, fill=key)) +
  geom_boxplot()
```

There are clearly a few observations for each variable which exceed 1.5 times
the IQR, the majority of which are in Calcium. We could further investigate 
whether the outliers have significant leverage using Cooke's distance, if outlier
removal was our intended goal. 


### Summary

The variables in the Glass dataset have a range of distributions from the highly
skewed to the nearly symmetrical, with an expect set of outliers. A sequence 
of transformations were applied to address some of these issues with a log transform
appearing to be most helpful. Some dimension reduction could be considered based on
desired interruptibility. All academic books advise PCA, but a simpler, more explabable 
reducer is generally advised in practice. I find a tree booster model to be much simpler
at identifying like predictors for removal over PCA. 

# Question KJ 3.2
## Question

The soybean data can also be found at the UC Irvine Machine Learning Repository.
Data were collected to predict disease in 683 soybeans. The 35 predictors are
mostly categorical and include information on the environmental conditions
(e.g., temperature, precipitation) and plant conditions (e.g., left spots,
mold growth). The outcome labels consist of 19 distinct classes. The data can be
loaded via:

```{r}
data(Soybean)
```

  (a) Investigate the frequency distributions for the categorical predictors.
  Are any of the distributions degenerate in the ways discussed earlier in this
  chapter?
  (b) Roughly 18\% of the data are missing. Are there particular predictors that
  are more likely to be missing? Is the pattern of missing data related to the
  classes?
  (c) Develop a strategy for handling missing data, either by eliminating
  predictors or imputation.

## Answer
### Part (a)

```{r, fig.width = 9L, fig.height = 8L, message=FALSE, warning=FALSE}
Soybean %>%
  gather() %>%
  ggplot(aes(value)) +
  geom_bar() +
  facet_wrap(~key, scales = 'free')
```


From the above, we see that there is no mathematically degenerate distribution,
in that every category has at least one instance of more than one variable type.
We should test the ratio of most frequent to the second most frequent 
observation in each category.

```{r, warning = FALSE}
name_col <- names(Soybean)
soy_ratios <- data.frame(variable = character(), ratio = double())
for (i in seq_len(dim(Soybean)[[2]])) {
  soy_ratios[i, 1] <- name_col[i]
  soy_ratios[i, 2] <- max(table(Soybean[, i])) /
    max(table(Soybean[, i])[-which.max(table(Soybean[, i]))])
}
soy_ratios[which(soy_ratios$ratio > 20), ]
```


Here we have three variable with a ratio that is higher than 20, 
the value suggested by Kuhn & Johnson. These would be categories for
consideration of removal or some other transform due to their imbalance.
I would imagine that `mycelium` is the most likely candidate for removal. 

### Part (b)

There clearly is a correlation between the Class and the missingness of data.

```{r, fig.width = 9L, fig.height = 10L, warning = F}
Soybean %>% 
  group_by(Class) %>%
  summarise_all(~sum(is.na(.))) %>%
  mutate(sum_missing = rowSums(across(where(is.numeric)))) %>%
  select(Class, sum_missing) %>%
  filter(sum_missing > 0)
```

It's clear that some classes have significant missing values
where the other 14 classes have none. 

### Part (c)

From the above we see that when a variable is missing, it tends to be missing for the entirety of its
class.

Lastly, of the 35 explanatory variables (not counting class itself), these
missing classes tend to be missing a lot of them

```{r}
Soybean %>% 
  group_by(Class) %>%
  summarise_all(~sum(is.na(.)))
```


All the classes with missing data are missing more than half of their data with
the exception of the `diaporthe-pod-&-stem-blight` class. These are likely to be
dropped from the dataset due to the fact that we would not want to impute the 
majority of the data. 

As for `diaporthe-pod-&-stem-blight`, we would like to see just how predictive the
missing features are in the remaining data.They may not be of much value. If they 
are, we could impute the 15 or so cases. 

